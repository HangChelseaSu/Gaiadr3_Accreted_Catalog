{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4282ad7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Login to gaia TAP server [astroquery.gaia.core]\n",
      "INFO: OK [astroquery.utils.tap.core]\n",
      "INFO: Login to gaia data server [astroquery.gaia.core]\n",
      "INFO: OK [astroquery.utils.tap.core]\n",
      "\n",
      "Starting query for starting value 0 and top 1000000 rows\n",
      "Job finished and result saved to /ocean/projects/phy210068p/hsu1/Ananke_datasets_training/Gaiadr3_data/gaia_download_newest_12062023DR3_6D_kinematics_0_to_1000000.csv\n",
      "\n",
      "Deleting job with id 1701889681134O\n",
      "INFO: Removed jobs: '['1701889681134O']'. [astroquery.utils.tap.core]\n",
      "\n",
      "Execution took 297.89169454574585 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#/usr/bin/env python\n",
    "\"\"\"\n",
    "Purpose of file: Download a subset (set by command line argument) of the rows of GAIA DR3, for the columns containing kinematic information \n",
    "suitable for parallelization via job submission system like slurm. \n",
    "\n",
    "Outputs:\n",
    "Writes the velocity data according to ADQL_base_script (defined in script). Default format for output files is csv.\n",
    "\n",
    "NOTES:\n",
    "-   On inspecting outputs: Check the number of lines (objects) via \"wc -l -c filename\" where you\n",
    "    replace filename, but only works as expected for csv files. In general to check file size in a human\n",
    "    readable format, type \"du -sh filename\" and in the output \"K\" is kilobytes, \"M\" is megabytes and so on.\n",
    "    For all files in folder do \"du -ha\"\n",
    "\n",
    "-   On unzipping: to unzip a folder recursively and overwrite the originals, use \"gunzip -r folder_name\"\n",
    "\"\"\"\n",
    "\n",
    "from astroquery.gaia import Gaia\n",
    "# import sys\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# For timing execution\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# ----------------- Set job parameters ----------------------\n",
    "\n",
    "# Define login details (necessary to avoid download limits)\n",
    "username = 'hsu01' # write your username\n",
    "password = 'Ch!13902986922'   # write your password\n",
    "Gaia.login(user=username, password=password)\n",
    "\n",
    "data_dir = \"/ocean/projects/phy210068p/hsu1/Ananke_datasets_training/Gaiadr3_data/gaia_download_newest_12062023\" # the folder with lots of storage where we'll save the files\n",
    "\n",
    "\n",
    "# Add TOP x after \"SELECT\" below to only get these columns for the first x objects (x a natural number) eg \"SELECT TOP 10 ...\"\n",
    "# The indentation isnt necessary in the ADQL script but it is good for readability\n",
    "\n",
    "ADQL_base_script = '''SELECT TOP %s\n",
    "                        source_id, ra, dec, l,b, parallax, parallax_error, pmra, pmra_error, pmdec, pmdec_error, \n",
    "                        parallax_pmra_corr, parallax_pmdec_corr, pmra_pmdec_corr, ruwe, radial_velocity, \n",
    "                        radial_velocity_error, parallax_over_error, phot_g_mean_mag,rv_expected_sig_to_noise\n",
    "                    FROM gaiadr3.gaia_source\n",
    "                    WHERE parallax_over_error > 10.0\n",
    "                    OFFSET %s\n",
    "                '''\n",
    "\n",
    "row_lim = 1000000\n",
    "offset = 1000000 * 0 # 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
    "\n",
    "# Print job info\n",
    "print(\"\\nStarting query for starting value {} and top {} rows\".format(offset,row_lim))\n",
    "\n",
    "# Define query and job name\n",
    "query = ADQL_base_script % (row_lim,offset)\n",
    "\n",
    "jobname = 'DR3_6D_kinematics_{}_to_{}'.format(offset,offset+row_lim) # Sets the output file name too\n",
    "output_filename = data_dir + jobname + '.csv'\n",
    "\n",
    "# Check if we already got this data\n",
    "if len(glob.glob(output_filename))==1:\n",
    "    print(\"Cancelling this job, \" + output_filename + \" already exists\")\n",
    "    print(\"\\nExiting execution...\")\n",
    "    sys.exit()\n",
    "    \n",
    "# Run job\n",
    "job=Gaia.launch_job_async(query, name=jobname, dump_to_file=True, output_format='csv',output_file = output_filename) # fits files are compressed\n",
    "job.get_results() \n",
    "\n",
    "print(\"Job finished and result saved to \" + output_filename + \"\\n\")\n",
    "\n",
    "# Delete the job from our cache (so we dont hit our quota)\n",
    "print(\"Deleting job with id {}\".format(job.jobid))\n",
    "Gaia.remove_jobs([job.jobid])\n",
    "\n",
    "# time execution\n",
    "print(\"\\nExecution took %s seconds\\n\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Login to gaia TAP server [astroquery.gaia.core]\n",
      "INFO: OK [astroquery.utils.tap.core]\n",
      "INFO: Login to gaia data server [astroquery.gaia.core]\n",
      "INFO: OK [astroquery.utils.tap.core]\n",
      "\n",
      "Starting query for starting value 3000000 and top 3000000 rows\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/jet/home/hsu1/data/Gaiadr3_Accreted_Catalog/gaia_data_download_debug.ipynb Cell 2\u001b[0m line \u001b[0;36m<cell line: 68>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2262726964676573322e7073632e656475222c2275736572223a2268737531227d/jet/home/hsu1/data/Gaiadr3_Accreted_Catalog/gaia_data_download_debug.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=64'>65</a>\u001b[0m     sys\u001b[39m.\u001b[39mexit()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2262726964676573322e7073632e656475222c2275736572223a2268737531227d/jet/home/hsu1/data/Gaiadr3_Accreted_Catalog/gaia_data_download_debug.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39m# Run job\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2262726964676573322e7073632e656475222c2275736572223a2268737531227d/jet/home/hsu1/data/Gaiadr3_Accreted_Catalog/gaia_data_download_debug.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m job\u001b[39m=\u001b[39mGaia\u001b[39m.\u001b[39;49mlaunch_job_async(query, name\u001b[39m=\u001b[39;49mjobname, dump_to_file\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, output_format\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcsv\u001b[39;49m\u001b[39m'\u001b[39;49m,output_file \u001b[39m=\u001b[39;49m output_filename) \u001b[39m# fits files are compressed\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2262726964676573322e7073632e656475222c2275736572223a2268737531227d/jet/home/hsu1/data/Gaiadr3_Accreted_Catalog/gaia_data_download_debug.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m job\u001b[39m.\u001b[39mget_results() \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2262726964676573322e7073632e656475222c2275736572223a2268737531227d/jet/home/hsu1/data/Gaiadr3_Accreted_Catalog/gaia_data_download_debug.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mJob finished and result saved to \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m output_filename \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/astroquery/gaia/core.py:900\u001b[0m, in \u001b[0;36mGaiaClass.launch_job_async\u001b[0;34m(self, query, name, output_file, output_format, verbose, dump_to_file, background, upload_resource, upload_table_name, autorun)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlaunch_job_async\u001b[39m(\u001b[39mself\u001b[39m, query, \u001b[39m*\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, output_file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    860\u001b[0m                      output_format\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvotable\u001b[39m\u001b[39m\"\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    861\u001b[0m                      dump_to_file\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, background\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    862\u001b[0m                      upload_resource\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, upload_table_name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    863\u001b[0m                      autorun\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    864\u001b[0m     \u001b[39m\"\"\"Launches an asynchronous job\u001b[39;00m\n\u001b[1;32m    865\u001b[0m \n\u001b[1;32m    866\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[39m    A Job object\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 900\u001b[0m     \u001b[39mreturn\u001b[39;00m TapPlus\u001b[39m.\u001b[39;49mlaunch_job_async(\u001b[39mself\u001b[39;49m, query\u001b[39m=\u001b[39;49mquery,\n\u001b[1;32m    901\u001b[0m                                     name\u001b[39m=\u001b[39;49mname,\n\u001b[1;32m    902\u001b[0m                                     output_file\u001b[39m=\u001b[39;49moutput_file,\n\u001b[1;32m    903\u001b[0m                                     output_format\u001b[39m=\u001b[39;49moutput_format,\n\u001b[1;32m    904\u001b[0m                                     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    905\u001b[0m                                     dump_to_file\u001b[39m=\u001b[39;49mdump_to_file,\n\u001b[1;32m    906\u001b[0m                                     background\u001b[39m=\u001b[39;49mbackground,\n\u001b[1;32m    907\u001b[0m                                     upload_resource\u001b[39m=\u001b[39;49mupload_resource,\n\u001b[1;32m    908\u001b[0m                                     upload_table_name\u001b[39m=\u001b[39;49mupload_table_name,\n\u001b[1;32m    909\u001b[0m                                     autorun\u001b[39m=\u001b[39;49mautorun)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/astroquery/utils/tap/core.py:467\u001b[0m, in \u001b[0;36mTap.launch_job_async\u001b[0;34m(self, query, name, output_file, output_format, verbose, dump_to_file, background, upload_resource, upload_table_name, autorun, maxrec)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[39m# saveResults or getResults will block (not background)\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[39mif\u001b[39;00m dump_to_file:\n\u001b[0;32m--> 467\u001b[0m     job\u001b[39m.\u001b[39;49msave_results(verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[1;32m    468\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m     job\u001b[39m.\u001b[39mget_results()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/astroquery/utils/tap/model/job.py:276\u001b[0m, in \u001b[0;36mJob.save_results\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    273\u001b[0m     log\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mNo results to save\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    274\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m     \u001b[39m# Async\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait_for_job_end(verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[1;32m    277\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconnHandler\u001b[39m.\u001b[39mexecute_tapget(\n\u001b[1;32m    278\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39masync/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjobid\u001b[39m}\u001b[39;00m\u001b[39m/results/result\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    279\u001b[0m     \u001b[39mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/astroquery/utils/tap/model/job.py:333\u001b[0m, in \u001b[0;36mJob.wait_for_job_end\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    331\u001b[0m     \u001b[39m# PENDING, QUEUED, EXECUTING, COMPLETED, ERROR, ABORTED, UNKNOWN,\u001b[39;00m\n\u001b[1;32m    332\u001b[0m     \u001b[39m# HELD, SUSPENDED, ARCHIVED:\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.5\u001b[39;49m)\n\u001b[1;32m    334\u001b[0m \u001b[39mreturn\u001b[39;00m currentResponse, lphase\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#/usr/bin/env python\n",
    "\"\"\"\n",
    "Purpose of file: Download a subset (set by command line argument) of the rows of GAIA DR3, for the columns containing kinematic information \n",
    "suitable for parallelization via job submission system like slurm. \n",
    "\n",
    "Outputs:\n",
    "Writes the velocity data according to ADQL_base_script (defined in script). Default format for output files is csv.\n",
    "\n",
    "NOTES:\n",
    "-   On inspecting outputs: Check the number of lines (objects) via \"wc -l -c filename\" where you\n",
    "    replace filename, but only works as expected for csv files. In general to check file size in a human\n",
    "    readable format, type \"du -sh filename\" and in the output \"K\" is kilobytes, \"M\" is megabytes and so on.\n",
    "    For all files in folder do \"du -ha\"\n",
    "\n",
    "-   On unzipping: to unzip a folder recursively and overwrite the originals, use \"gunzip -r folder_name\"\n",
    "\"\"\"\n",
    "\n",
    "from astroquery.gaia import Gaia\n",
    "# import sys\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# For timing execution\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# ----------------- Set job parameters ----------------------\n",
    "\n",
    "# Define login details (necessary to avoid download limits)\n",
    "username = 'hsu01' # write your username\n",
    "password = 'Ch!13902986922'   # write your password\n",
    "Gaia.login(user=username, password=password)\n",
    "\n",
    "data_dir = \"/ocean/projects/phy210068p/hsu1/Ananke_datasets_training/Gaiadr3_data/gaia_download_newest_12062023\" # the folder with lots of storage where we'll save the files\n",
    "\n",
    "\n",
    "# Add TOP x after \"SELECT\" below to only get these columns for the first x objects (x a natural number) eg \"SELECT TOP 10 ...\"\n",
    "# The indentation isnt necessary in the ADQL script but it is good for readability\n",
    "\n",
    "ADQL_base_script = '''SELECT TOP %s\n",
    "                        source_id, ra, dec, l,b, parallax, parallax_error, pmra, pmra_error, pmdec, pmdec_error, \n",
    "                        parallax_pmra_corr, parallax_pmdec_corr, pmra_pmdec_corr, ruwe, radial_velocity, \n",
    "                        radial_velocity_error, parallax_over_error, phot_g_mean_mag,rv_expected_sig_to_noise\n",
    "                    FROM gaiadr3.gaia_source\n",
    "                    WHERE parallax_over_error > 10.0\n",
    "                    OFFSET %s\n",
    "                '''\n",
    "\n",
    "row_lim = 3000000\n",
    "offset = 3000000 * 1 # 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
    "\n",
    "# Print job info\n",
    "print(\"\\nStarting query for starting value {} and top {} rows\".format(offset,row_lim))\n",
    "\n",
    "# Define query and job name\n",
    "query = ADQL_base_script % (row_lim,offset)\n",
    "\n",
    "jobname = 'DR3_6D_kinematics_{}_to_{}'.format(offset,offset+row_lim) # Sets the output file name too\n",
    "output_filename = data_dir + jobname + '.csv'\n",
    "\n",
    "# Check if we already got this data\n",
    "if len(glob.glob(output_filename))==1:\n",
    "    print(\"Cancelling this job, \" + output_filename + \" already exists\")\n",
    "    print(\"\\nExiting execution...\")\n",
    "    sys.exit()\n",
    "    \n",
    "# Run job\n",
    "job=Gaia.launch_job_async(query, name=jobname, dump_to_file=True, output_format='csv',output_file = output_filename) # fits files are compressed\n",
    "job.get_results() \n",
    "\n",
    "print(\"Job finished and result saved to \" + output_filename + \"\\n\")\n",
    "\n",
    "# Delete the job from our cache (so we dont hit our quota)\n",
    "print(\"Deleting job with id {}\".format(job.jobid))\n",
    "Gaia.remove_jobs([job.jobid])\n",
    "\n",
    "# time execution\n",
    "print(\"\\nExecution took %s seconds\\n\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/usr/bin/env python\n",
    "\"\"\"\n",
    "Purpose of file: Download a subset (set by command line argument) of the rows of GAIA DR3, for the columns containing kinematic information \n",
    "suitable for parallelization via job submission system like slurm. \n",
    "\n",
    "Outputs:\n",
    "Writes the velocity data according to ADQL_base_script (defined in script). Default format for output files is csv.\n",
    "\n",
    "NOTES:\n",
    "-   On inspecting outputs: Check the number of lines (objects) via \"wc -l -c filename\" where you\n",
    "    replace filename, but only works as expected for csv files. In general to check file size in a human\n",
    "    readable format, type \"du -sh filename\" and in the output \"K\" is kilobytes, \"M\" is megabytes and so on.\n",
    "    For all files in folder do \"du -ha\"\n",
    "\n",
    "-   On unzipping: to unzip a folder recursively and overwrite the originals, use \"gunzip -r folder_name\"\n",
    "\"\"\"\n",
    "\n",
    "from astroquery.gaia import Gaia\n",
    "# import sys\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# For timing execution\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# ----------------- Set job parameters ----------------------\n",
    "\n",
    "# Define login details (necessary to avoid download limits)\n",
    "username = 'hsu01' # write your username\n",
    "password = 'Ch!13902986922'   # write your password\n",
    "Gaia.login(user=username, password=password)\n",
    "\n",
    "data_dir = \"/ocean/projects/phy210068p/hsu1/Ananke_datasets_training/Gaiadr3_data/gaia_download_newest_12062023\" # the folder with lots of storage where we'll save the files\n",
    "\n",
    "\n",
    "# Add TOP x after \"SELECT\" below to only get these columns for the first x objects (x a natural number) eg \"SELECT TOP 10 ...\"\n",
    "# The indentation isnt necessary in the ADQL script but it is good for readability\n",
    "\n",
    "ADQL_base_script = '''SELECT TOP %s\n",
    "                        source_id, ra, dec, l,b, parallax, parallax_error, pmra, pmra_error, pmdec, pmdec_error, \n",
    "                        parallax_pmra_corr, parallax_pmdec_corr, pmra_pmdec_corr, ruwe, radial_velocity, \n",
    "                        radial_velocity_error, parallax_over_error, phot_g_mean_mag,rv_expected_sig_to_noise\n",
    "                    FROM gaiadr3.gaia_source\n",
    "                    WHERE parallax_over_error > 10.0\n",
    "                    OFFSET %s\n",
    "                '''\n",
    "\n",
    "row_lim = 3000000\n",
    "offset = 3000000 * 2 # 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
    "\n",
    "# Print job info\n",
    "print(\"\\nStarting query for starting value {} and top {} rows\".format(offset,row_lim))\n",
    "\n",
    "# Define query and job name\n",
    "query = ADQL_base_script % (row_lim,offset)\n",
    "\n",
    "jobname = 'DR3_6D_kinematics_{}_to_{}'.format(offset,offset+row_lim) # Sets the output file name too\n",
    "output_filename = data_dir + jobname + '.csv'\n",
    "\n",
    "# Check if we already got this data\n",
    "if len(glob.glob(output_filename))==1:\n",
    "    print(\"Cancelling this job, \" + output_filename + \" already exists\")\n",
    "    print(\"\\nExiting execution...\")\n",
    "    sys.exit()\n",
    "    \n",
    "# Run job\n",
    "job=Gaia.launch_job_async(query, name=jobname, dump_to_file=True, output_format='csv',output_file = output_filename) # fits files are compressed\n",
    "job.get_results() \n",
    "\n",
    "print(\"Job finished and result saved to \" + output_filename + \"\\n\")\n",
    "\n",
    "# Delete the job from our cache (so we dont hit our quota)\n",
    "print(\"Deleting job with id {}\".format(job.jobid))\n",
    "Gaia.remove_jobs([job.jobid])\n",
    "\n",
    "# time execution\n",
    "print(\"\\nExecution took %s seconds\\n\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/usr/bin/env python\n",
    "\"\"\"\n",
    "Purpose of file: Download a subset (set by command line argument) of the rows of GAIA DR3, for the columns containing kinematic information \n",
    "suitable for parallelization via job submission system like slurm. \n",
    "\n",
    "Outputs:\n",
    "Writes the velocity data according to ADQL_base_script (defined in script). Default format for output files is csv.\n",
    "\n",
    "NOTES:\n",
    "-   On inspecting outputs: Check the number of lines (objects) via \"wc -l -c filename\" where you\n",
    "    replace filename, but only works as expected for csv files. In general to check file size in a human\n",
    "    readable format, type \"du -sh filename\" and in the output \"K\" is kilobytes, \"M\" is megabytes and so on.\n",
    "    For all files in folder do \"du -ha\"\n",
    "\n",
    "-   On unzipping: to unzip a folder recursively and overwrite the originals, use \"gunzip -r folder_name\"\n",
    "\"\"\"\n",
    "\n",
    "from astroquery.gaia import Gaia\n",
    "# import sys\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# For timing execution\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# ----------------- Set job parameters ----------------------\n",
    "\n",
    "# Define login details (necessary to avoid download limits)\n",
    "username = 'hsu01' # write your username\n",
    "password = 'Ch!13902986922'   # write your password\n",
    "Gaia.login(user=username, password=password)\n",
    "\n",
    "data_dir = \"/ocean/projects/phy210068p/hsu1/Ananke_datasets_training/Gaiadr3_data/gaia_download_newest_12062023\" # the folder with lots of storage where we'll save the files\n",
    "\n",
    "\n",
    "# Add TOP x after \"SELECT\" below to only get these columns for the first x objects (x a natural number) eg \"SELECT TOP 10 ...\"\n",
    "# The indentation isnt necessary in the ADQL script but it is good for readability\n",
    "\n",
    "ADQL_base_script = '''SELECT TOP %s\n",
    "                        source_id, ra, dec, l,b, parallax, parallax_error, pmra, pmra_error, pmdec, pmdec_error, \n",
    "                        parallax_pmra_corr, parallax_pmdec_corr, pmra_pmdec_corr, ruwe, radial_velocity, \n",
    "                        radial_velocity_error, parallax_over_error, phot_g_mean_mag,rv_expected_sig_to_noise\n",
    "                    FROM gaiadr3.gaia_source\n",
    "                    WHERE parallax_over_error > 10.0\n",
    "                    OFFSET %s\n",
    "                '''\n",
    "\n",
    "row_lim = 3000000\n",
    "offset = 3000000 * 3 # 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
    "\n",
    "# Print job info\n",
    "print(\"\\nStarting query for starting value {} and top {} rows\".format(offset,row_lim))\n",
    "\n",
    "# Define query and job name\n",
    "query = ADQL_base_script % (row_lim,offset)\n",
    "\n",
    "jobname = 'DR3_6D_kinematics_{}_to_{}'.format(offset,offset+row_lim) # Sets the output file name too\n",
    "output_filename = data_dir + jobname + '.csv'\n",
    "\n",
    "# Check if we already got this data\n",
    "if len(glob.glob(output_filename))==1:\n",
    "    print(\"Cancelling this job, \" + output_filename + \" already exists\")\n",
    "    print(\"\\nExiting execution...\")\n",
    "    sys.exit()\n",
    "    \n",
    "# Run job\n",
    "job=Gaia.launch_job_async(query, name=jobname, dump_to_file=True, output_format='csv',output_file = output_filename) # fits files are compressed\n",
    "job.get_results() \n",
    "\n",
    "print(\"Job finished and result saved to \" + output_filename + \"\\n\")\n",
    "\n",
    "# Delete the job from our cache (so we dont hit our quota)\n",
    "print(\"Deleting job with id {}\".format(job.jobid))\n",
    "Gaia.remove_jobs([job.jobid])\n",
    "\n",
    "# time execution\n",
    "print(\"\\nExecution took %s seconds\\n\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/usr/bin/env python\n",
    "\"\"\"\n",
    "Purpose of file: Download a subset (set by command line argument) of the rows of GAIA DR3, for the columns containing kinematic information \n",
    "suitable for parallelization via job submission system like slurm. \n",
    "\n",
    "Outputs:\n",
    "Writes the velocity data according to ADQL_base_script (defined in script). Default format for output files is csv.\n",
    "\n",
    "NOTES:\n",
    "-   On inspecting outputs: Check the number of lines (objects) via \"wc -l -c filename\" where you\n",
    "    replace filename, but only works as expected for csv files. In general to check file size in a human\n",
    "    readable format, type \"du -sh filename\" and in the output \"K\" is kilobytes, \"M\" is megabytes and so on.\n",
    "    For all files in folder do \"du -ha\"\n",
    "\n",
    "-   On unzipping: to unzip a folder recursively and overwrite the originals, use \"gunzip -r folder_name\"\n",
    "\"\"\"\n",
    "\n",
    "from astroquery.gaia import Gaia\n",
    "# import sys\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# For timing execution\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# ----------------- Set job parameters ----------------------\n",
    "\n",
    "# Define login details (necessary to avoid download limits)\n",
    "username = 'hsu01' # write your username\n",
    "password = 'Ch!13902986922'   # write your password\n",
    "Gaia.login(user=username, password=password)\n",
    "\n",
    "data_dir = \"/ocean/projects/phy210068p/hsu1/Ananke_datasets_training/Gaiadr3_data/gaia_download_newest_12062023\" # the folder with lots of storage where we'll save the files\n",
    "\n",
    "\n",
    "# Add TOP x after \"SELECT\" below to only get these columns for the first x objects (x a natural number) eg \"SELECT TOP 10 ...\"\n",
    "# The indentation isnt necessary in the ADQL script but it is good for readability\n",
    "\n",
    "ADQL_base_script = '''SELECT TOP %s\n",
    "                        source_id, ra, dec, l,b, parallax, parallax_error, pmra, pmra_error, pmdec, pmdec_error, \n",
    "                        parallax_pmra_corr, parallax_pmdec_corr, pmra_pmdec_corr, ruwe, radial_velocity, \n",
    "                        radial_velocity_error, parallax_over_error, phot_g_mean_mag,rv_expected_sig_to_noise\n",
    "                    FROM gaiadr3.gaia_source\n",
    "                    WHERE parallax_over_error > 10.0\n",
    "                    OFFSET %s\n",
    "                '''\n",
    "\n",
    "row_lim = 3000000\n",
    "offset = 3000000 * 4 # 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
    "\n",
    "# Print job info\n",
    "print(\"\\nStarting query for starting value {} and top {} rows\".format(offset,row_lim))\n",
    "\n",
    "# Define query and job name\n",
    "query = ADQL_base_script % (row_lim,offset)\n",
    "\n",
    "jobname = 'DR3_6D_kinematics_{}_to_{}'.format(offset,offset+row_lim) # Sets the output file name too\n",
    "output_filename = data_dir + jobname + '.csv'\n",
    "\n",
    "# Check if we already got this data\n",
    "if len(glob.glob(output_filename))==1:\n",
    "    print(\"Cancelling this job, \" + output_filename + \" already exists\")\n",
    "    print(\"\\nExiting execution...\")\n",
    "    sys.exit()\n",
    "    \n",
    "# Run job\n",
    "job=Gaia.launch_job_async(query, name=jobname, dump_to_file=True, output_format='csv',output_file = output_filename) # fits files are compressed\n",
    "job.get_results() \n",
    "\n",
    "print(\"Job finished and result saved to \" + output_filename + \"\\n\")\n",
    "\n",
    "# Delete the job from our cache (so we dont hit our quota)\n",
    "print(\"Deleting job with id {}\".format(job.jobid))\n",
    "Gaia.remove_jobs([job.jobid])\n",
    "\n",
    "# time execution\n",
    "print(\"\\nExecution took %s seconds\\n\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/usr/bin/env python\n",
    "\"\"\"\n",
    "Purpose of file: Download a subset (set by command line argument) of the rows of GAIA DR3, for the columns containing kinematic information \n",
    "suitable for parallelization via job submission system like slurm. \n",
    "\n",
    "Outputs:\n",
    "Writes the velocity data according to ADQL_base_script (defined in script). Default format for output files is csv.\n",
    "\n",
    "NOTES:\n",
    "-   On inspecting outputs: Check the number of lines (objects) via \"wc -l -c filename\" where you\n",
    "    replace filename, but only works as expected for csv files. In general to check file size in a human\n",
    "    readable format, type \"du -sh filename\" and in the output \"K\" is kilobytes, \"M\" is megabytes and so on.\n",
    "    For all files in folder do \"du -ha\"\n",
    "\n",
    "-   On unzipping: to unzip a folder recursively and overwrite the originals, use \"gunzip -r folder_name\"\n",
    "\"\"\"\n",
    "\n",
    "from astroquery.gaia import Gaia\n",
    "# import sys\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# For timing execution\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# ----------------- Set job parameters ----------------------\n",
    "\n",
    "# Define login details (necessary to avoid download limits)\n",
    "username = 'hsu01' # write your username\n",
    "password = 'Ch!13902986922'   # write your password\n",
    "Gaia.login(user=username, password=password)\n",
    "\n",
    "data_dir = \"/ocean/projects/phy210068p/hsu1/Ananke_datasets_training/Gaiadr3_data/gaia_download_newest_12062023\" # the folder with lots of storage where we'll save the files\n",
    "\n",
    "\n",
    "# Add TOP x after \"SELECT\" below to only get these columns for the first x objects (x a natural number) eg \"SELECT TOP 10 ...\"\n",
    "# The indentation isnt necessary in the ADQL script but it is good for readability\n",
    "\n",
    "ADQL_base_script = '''SELECT TOP %s\n",
    "                        source_id, ra, dec, l,b, parallax, parallax_error, pmra, pmra_error, pmdec, pmdec_error, \n",
    "                        parallax_pmra_corr, parallax_pmdec_corr, pmra_pmdec_corr, ruwe, radial_velocity, \n",
    "                        radial_velocity_error, parallax_over_error, phot_g_mean_mag,rv_expected_sig_to_noise\n",
    "                    FROM gaiadr3.gaia_source\n",
    "                    WHERE parallax_over_error > 10.0\n",
    "                    OFFSET %s\n",
    "                '''\n",
    "\n",
    "row_lim = 3000000\n",
    "offset = 3000000 * 5 # 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
    "\n",
    "# Print job info\n",
    "print(\"\\nStarting query for starting value {} and top {} rows\".format(offset,row_lim))\n",
    "\n",
    "# Define query and job name\n",
    "query = ADQL_base_script % (row_lim,offset)\n",
    "\n",
    "jobname = 'DR3_6D_kinematics_{}_to_{}'.format(offset,offset+row_lim) # Sets the output file name too\n",
    "output_filename = data_dir + jobname + '.csv'\n",
    "\n",
    "# Check if we already got this data\n",
    "if len(glob.glob(output_filename))==1:\n",
    "    print(\"Cancelling this job, \" + output_filename + \" already exists\")\n",
    "    print(\"\\nExiting execution...\")\n",
    "    sys.exit()\n",
    "    \n",
    "# Run job\n",
    "job=Gaia.launch_job_async(query, name=jobname, dump_to_file=True, output_format='csv',output_file = output_filename) # fits files are compressed\n",
    "job.get_results() \n",
    "\n",
    "print(\"Job finished and result saved to \" + output_filename + \"\\n\")\n",
    "\n",
    "# Delete the job from our cache (so we dont hit our quota)\n",
    "print(\"Deleting job with id {}\".format(job.jobid))\n",
    "Gaia.remove_jobs([job.jobid])\n",
    "\n",
    "# time execution\n",
    "print(\"\\nExecution took %s seconds\\n\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/usr/bin/env python\n",
    "\"\"\"\n",
    "Purpose of file: Download a subset (set by command line argument) of the rows of GAIA DR3, for the columns containing kinematic information \n",
    "suitable for parallelization via job submission system like slurm. \n",
    "\n",
    "Outputs:\n",
    "Writes the velocity data according to ADQL_base_script (defined in script). Default format for output files is csv.\n",
    "\n",
    "NOTES:\n",
    "-   On inspecting outputs: Check the number of lines (objects) via \"wc -l -c filename\" where you\n",
    "    replace filename, but only works as expected for csv files. In general to check file size in a human\n",
    "    readable format, type \"du -sh filename\" and in the output \"K\" is kilobytes, \"M\" is megabytes and so on.\n",
    "    For all files in folder do \"du -ha\"\n",
    "\n",
    "-   On unzipping: to unzip a folder recursively and overwrite the originals, use \"gunzip -r folder_name\"\n",
    "\"\"\"\n",
    "\n",
    "from astroquery.gaia import Gaia\n",
    "# import sys\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# For timing execution\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# ----------------- Set job parameters ----------------------\n",
    "\n",
    "# Define login details (necessary to avoid download limits)\n",
    "username = 'hsu01' # write your username\n",
    "password = 'Ch!13902986922'   # write your password\n",
    "Gaia.login(user=username, password=password)\n",
    "\n",
    "data_dir = \"/ocean/projects/phy210068p/hsu1/Ananke_datasets_training/Gaiadr3_data/gaia_download_newest_12062023\" # the folder with lots of storage where we'll save the files\n",
    "\n",
    "\n",
    "# Add TOP x after \"SELECT\" below to only get these columns for the first x objects (x a natural number) eg \"SELECT TOP 10 ...\"\n",
    "# The indentation isnt necessary in the ADQL script but it is good for readability\n",
    "\n",
    "ADQL_base_script = '''SELECT TOP %s\n",
    "                        source_id, ra, dec, l,b, parallax, parallax_error, pmra, pmra_error, pmdec, pmdec_error, \n",
    "                        parallax_pmra_corr, parallax_pmdec_corr, pmra_pmdec_corr, ruwe, radial_velocity, \n",
    "                        radial_velocity_error, parallax_over_error, phot_g_mean_mag,rv_expected_sig_to_noise\n",
    "                    FROM gaiadr3.gaia_source\n",
    "                    WHERE parallax_over_error > 10.0\n",
    "                    OFFSET %s\n",
    "                '''\n",
    "\n",
    "row_lim = 3000000\n",
    "offset = 3000000 * 6 # 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
    "\n",
    "# Print job info\n",
    "print(\"\\nStarting query for starting value {} and top {} rows\".format(offset,row_lim))\n",
    "\n",
    "# Define query and job name\n",
    "query = ADQL_base_script % (row_lim,offset)\n",
    "\n",
    "jobname = 'DR3_6D_kinematics_{}_to_{}'.format(offset,offset+row_lim) # Sets the output file name too\n",
    "output_filename = data_dir + jobname + '.csv'\n",
    "\n",
    "# Check if we already got this data\n",
    "if len(glob.glob(output_filename))==1:\n",
    "    print(\"Cancelling this job, \" + output_filename + \" already exists\")\n",
    "    print(\"\\nExiting execution...\")\n",
    "    sys.exit()\n",
    "    \n",
    "# Run job\n",
    "job=Gaia.launch_job_async(query, name=jobname, dump_to_file=True, output_format='csv',output_file = output_filename) # fits files are compressed\n",
    "job.get_results() \n",
    "\n",
    "print(\"Job finished and result saved to \" + output_filename + \"\\n\")\n",
    "\n",
    "# Delete the job from our cache (so we dont hit our quota)\n",
    "print(\"Deleting job with id {}\".format(job.jobid))\n",
    "Gaia.remove_jobs([job.jobid])\n",
    "\n",
    "# time execution\n",
    "print(\"\\nExecution took %s seconds\\n\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/usr/bin/env python\n",
    "\"\"\"\n",
    "Purpose of file: Download a subset (set by command line argument) of the rows of GAIA DR3, for the columns containing kinematic information \n",
    "suitable for parallelization via job submission system like slurm. \n",
    "\n",
    "Outputs:\n",
    "Writes the velocity data according to ADQL_base_script (defined in script). Default format for output files is csv.\n",
    "\n",
    "NOTES:\n",
    "-   On inspecting outputs: Check the number of lines (objects) via \"wc -l -c filename\" where you\n",
    "    replace filename, but only works as expected for csv files. In general to check file size in a human\n",
    "    readable format, type \"du -sh filename\" and in the output \"K\" is kilobytes, \"M\" is megabytes and so on.\n",
    "    For all files in folder do \"du -ha\"\n",
    "\n",
    "-   On unzipping: to unzip a folder recursively and overwrite the originals, use \"gunzip -r folder_name\"\n",
    "\"\"\"\n",
    "\n",
    "from astroquery.gaia import Gaia\n",
    "# import sys\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# For timing execution\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# ----------------- Set job parameters ----------------------\n",
    "\n",
    "# Define login details (necessary to avoid download limits)\n",
    "username = 'hsu01' # write your username\n",
    "password = 'Ch!13902986922'   # write your password\n",
    "Gaia.login(user=username, password=password)\n",
    "\n",
    "data_dir = \"/ocean/projects/phy210068p/hsu1/Ananke_datasets_training/Gaiadr3_data/gaia_download_newest_12062023\" # the folder with lots of storage where we'll save the files\n",
    "\n",
    "\n",
    "# Add TOP x after \"SELECT\" below to only get these columns for the first x objects (x a natural number) eg \"SELECT TOP 10 ...\"\n",
    "# The indentation isnt necessary in the ADQL script but it is good for readability\n",
    "\n",
    "ADQL_base_script = '''SELECT TOP %s\n",
    "                        source_id, ra, dec, l,b, parallax, parallax_error, pmra, pmra_error, pmdec, pmdec_error, \n",
    "                        parallax_pmra_corr, parallax_pmdec_corr, pmra_pmdec_corr, ruwe, radial_velocity, \n",
    "                        radial_velocity_error, parallax_over_error, phot_g_mean_mag,rv_expected_sig_to_noise\n",
    "                    FROM gaiadr3.gaia_source\n",
    "                    WHERE parallax_over_error > 10.0\n",
    "                    OFFSET %s\n",
    "                '''\n",
    "\n",
    "row_lim = 3000000\n",
    "offset = 3000000 * 7 # 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
    "\n",
    "# Print job info\n",
    "print(\"\\nStarting query for starting value {} and top {} rows\".format(offset,row_lim))\n",
    "\n",
    "# Define query and job name\n",
    "query = ADQL_base_script % (row_lim,offset)\n",
    "\n",
    "jobname = 'DR3_6D_kinematics_{}_to_{}'.format(offset,offset+row_lim) # Sets the output file name too\n",
    "output_filename = data_dir + jobname + '.csv'\n",
    "\n",
    "# Check if we already got this data\n",
    "if len(glob.glob(output_filename))==1:\n",
    "    print(\"Cancelling this job, \" + output_filename + \" already exists\")\n",
    "    print(\"\\nExiting execution...\")\n",
    "    sys.exit()\n",
    "    \n",
    "# Run job\n",
    "job=Gaia.launch_job_async(query, name=jobname, dump_to_file=True, output_format='csv',output_file = output_filename) # fits files are compressed\n",
    "job.get_results() \n",
    "\n",
    "print(\"Job finished and result saved to \" + output_filename + \"\\n\")\n",
    "\n",
    "# Delete the job from our cache (so we dont hit our quota)\n",
    "print(\"Deleting job with id {}\".format(job.jobid))\n",
    "Gaia.remove_jobs([job.jobid])\n",
    "\n",
    "# time execution\n",
    "print(\"\\nExecution took %s seconds\\n\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/usr/bin/env python\n",
    "\"\"\"\n",
    "Purpose of file: Download a subset (set by command line argument) of the rows of GAIA DR3, for the columns containing kinematic information \n",
    "suitable for parallelization via job submission system like slurm. \n",
    "\n",
    "Outputs:\n",
    "Writes the velocity data according to ADQL_base_script (defined in script). Default format for output files is csv.\n",
    "\n",
    "NOTES:\n",
    "-   On inspecting outputs: Check the number of lines (objects) via \"wc -l -c filename\" where you\n",
    "    replace filename, but only works as expected for csv files. In general to check file size in a human\n",
    "    readable format, type \"du -sh filename\" and in the output \"K\" is kilobytes, \"M\" is megabytes and so on.\n",
    "    For all files in folder do \"du -ha\"\n",
    "\n",
    "-   On unzipping: to unzip a folder recursively and overwrite the originals, use \"gunzip -r folder_name\"\n",
    "\"\"\"\n",
    "\n",
    "from astroquery.gaia import Gaia\n",
    "# import sys\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# For timing execution\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# ----------------- Set job parameters ----------------------\n",
    "\n",
    "# Define login details (necessary to avoid download limits)\n",
    "username = 'hsu01' # write your username\n",
    "password = 'Ch!13902986922'   # write your password\n",
    "Gaia.login(user=username, password=password)\n",
    "\n",
    "data_dir = \"/ocean/projects/phy210068p/hsu1/Ananke_datasets_training/Gaiadr3_data/gaia_download_newest_12062023\" # the folder with lots of storage where we'll save the files\n",
    "\n",
    "\n",
    "# Add TOP x after \"SELECT\" below to only get these columns for the first x objects (x a natural number) eg \"SELECT TOP 10 ...\"\n",
    "# The indentation isnt necessary in the ADQL script but it is good for readability\n",
    "\n",
    "ADQL_base_script = '''SELECT TOP %s\n",
    "                        source_id, ra, dec, l,b, parallax, parallax_error, pmra, pmra_error, pmdec, pmdec_error, \n",
    "                        parallax_pmra_corr, parallax_pmdec_corr, pmra_pmdec_corr, ruwe, radial_velocity, \n",
    "                        radial_velocity_error, parallax_over_error, phot_g_mean_mag,rv_expected_sig_to_noise\n",
    "                    FROM gaiadr3.gaia_source\n",
    "                    WHERE parallax_over_error > 10.0\n",
    "                    OFFSET %s\n",
    "                '''\n",
    "\n",
    "row_lim = 3000000\n",
    "offset = 3000000 * 8 # 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
    "\n",
    "# Print job info\n",
    "print(\"\\nStarting query for starting value {} and top {} rows\".format(offset,row_lim))\n",
    "\n",
    "# Define query and job name\n",
    "query = ADQL_base_script % (row_lim,offset)\n",
    "\n",
    "jobname = 'DR3_6D_kinematics_{}_to_{}'.format(offset,offset+row_lim) # Sets the output file name too\n",
    "output_filename = data_dir + jobname + '.csv'\n",
    "\n",
    "# Check if we already got this data\n",
    "if len(glob.glob(output_filename))==1:\n",
    "    print(\"Cancelling this job, \" + output_filename + \" already exists\")\n",
    "    print(\"\\nExiting execution...\")\n",
    "    sys.exit()\n",
    "    \n",
    "# Run job\n",
    "job=Gaia.launch_job_async(query, name=jobname, dump_to_file=True, output_format='csv',output_file = output_filename) # fits files are compressed\n",
    "job.get_results() \n",
    "\n",
    "print(\"Job finished and result saved to \" + output_filename + \"\\n\")\n",
    "\n",
    "# Delete the job from our cache (so we dont hit our quota)\n",
    "print(\"Deleting job with id {}\".format(job.jobid))\n",
    "Gaia.remove_jobs([job.jobid])\n",
    "\n",
    "# time execution\n",
    "print(\"\\nExecution took %s seconds\\n\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/usr/bin/env python\n",
    "\"\"\"\n",
    "Purpose of file: Download a subset (set by command line argument) of the rows of GAIA DR3, for the columns containing kinematic information \n",
    "suitable for parallelization via job submission system like slurm. \n",
    "\n",
    "Outputs:\n",
    "Writes the velocity data according to ADQL_base_script (defined in script). Default format for output files is csv.\n",
    "\n",
    "NOTES:\n",
    "-   On inspecting outputs: Check the number of lines (objects) via \"wc -l -c filename\" where you\n",
    "    replace filename, but only works as expected for csv files. In general to check file size in a human\n",
    "    readable format, type \"du -sh filename\" and in the output \"K\" is kilobytes, \"M\" is megabytes and so on.\n",
    "    For all files in folder do \"du -ha\"\n",
    "\n",
    "-   On unzipping: to unzip a folder recursively and overwrite the originals, use \"gunzip -r folder_name\"\n",
    "\"\"\"\n",
    "\n",
    "from astroquery.gaia import Gaia\n",
    "# import sys\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# For timing execution\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "# ----------------- Set job parameters ----------------------\n",
    "\n",
    "# Define login details (necessary to avoid download limits)\n",
    "username = 'hsu01' # write your username\n",
    "password = 'Ch!13902986922'   # write your password\n",
    "Gaia.login(user=username, password=password)\n",
    "\n",
    "data_dir = \"/ocean/projects/phy210068p/hsu1/Ananke_datasets_training/Gaiadr3_data/gaia_download_newest_12062023\" # the folder with lots of storage where we'll save the files\n",
    "\n",
    "\n",
    "# Add TOP x after \"SELECT\" below to only get these columns for the first x objects (x a natural number) eg \"SELECT TOP 10 ...\"\n",
    "# The indentation isnt necessary in the ADQL script but it is good for readability\n",
    "\n",
    "ADQL_base_script = '''SELECT TOP %s\n",
    "                        source_id, ra, dec, l,b, parallax, parallax_error, pmra, pmra_error, pmdec, pmdec_error, \n",
    "                        parallax_pmra_corr, parallax_pmdec_corr, pmra_pmdec_corr, ruwe, radial_velocity, \n",
    "                        radial_velocity_error, parallax_over_error, phot_g_mean_mag,rv_expected_sig_to_noise\n",
    "                    FROM gaiadr3.gaia_source\n",
    "                    WHERE parallax_over_error > 10.0\n",
    "                    OFFSET %s\n",
    "                '''\n",
    "\n",
    "row_lim = 3000000\n",
    "offset = 3000000 * 9 # 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n",
    "\n",
    "# Print job info\n",
    "print(\"\\nStarting query for starting value {} and top {} rows\".format(offset,row_lim))\n",
    "\n",
    "# Define query and job name\n",
    "query = ADQL_base_script % (row_lim,offset)\n",
    "\n",
    "jobname = 'DR3_6D_kinematics_{}_to_{}'.format(offset,offset+row_lim) # Sets the output file name too\n",
    "output_filename = data_dir + jobname + '.csv'\n",
    "\n",
    "# Check if we already got this data\n",
    "if len(glob.glob(output_filename))==1:\n",
    "    print(\"Cancelling this job, \" + output_filename + \" already exists\")\n",
    "    print(\"\\nExiting execution...\")\n",
    "    sys.exit()\n",
    "    \n",
    "# Run job\n",
    "job=Gaia.launch_job_async(query, name=jobname, dump_to_file=True, output_format='csv',output_file = output_filename) # fits files are compressed\n",
    "job.get_results() \n",
    "\n",
    "print(\"Job finished and result saved to \" + output_filename + \"\\n\")\n",
    "\n",
    "# Delete the job from our cache (so we dont hit our quota)\n",
    "print(\"Deleting job with id {}\".format(job.jobid))\n",
    "Gaia.remove_jobs([job.jobid])\n",
    "\n",
    "# time execution\n",
    "print(\"\\nExecution took %s seconds\\n\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
