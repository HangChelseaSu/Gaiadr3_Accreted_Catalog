{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e839551e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_64280/139996037.py:10: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from IPython.core.display import display\n",
    "from collections.abc import Mapping\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "import torchmetrics\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-colorblind')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e4dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#User Input\n",
    "sim = 'DR3_lsr012' #input(\"DR2 or DR3: \")\n",
    "dim = '4D' #input(\"Input how many dimensions are needed: \")\n",
    "galaxy = 'Gaia' #input(\"Use m12i or m12f data: \")\n",
    "transfer = True #bool(input(\"Transfer learning (True or False): \"))\n",
    "if transfer == True:\n",
    "    transfer_galaxy = 'm12i' #input(\"Which galaxy parameters for transfer learning: \")\n",
    "\n",
    "# Training data\n",
    "if dim == '4D':\n",
    "    x_keys = ['ra', 'dec', 'pmra', 'pmdec']\n",
    "elif dim == '5D':\n",
    "    x_keys = ['ra', 'dec', 'pmra', 'pmdec', 'parallax']\n",
    "elif dim == '6D':\n",
    "    x_keys = ['ra', 'dec', 'pmra', 'pmdec', 'parallax', 'radial_velocity']\n",
    "elif dim == '7D':\n",
    "    x_keys = ['ra', 'dec', 'pmra', 'pmdec', 'parallax', 'radial_velocity', 'feh']\n",
    "elif dim == '9D':\n",
    "    x_keys = ['ra', 'dec', 'pmra', 'pmdec', 'parallax', 'radial_velocity', 'Jr', 'Jphi', 'Jz']\n",
    "elif dim == '10D':\n",
    "    x_keys = ['ra', 'dec', 'pmra', 'pmdec', 'parallax', 'radial_velocity', 'Jr', 'Jphi', 'Jz', 'feh']\n",
    "    \n",
    "y_key = 'is_accreted'\n",
    "\n",
    "# Directories\n",
    "path = '/ocean/projects/phy210068p/hsu1/Ananke_datasets_training/GaiaDR3_data_reduced.hdf5'\n",
    "out_dir = '/ocean/projects/phy210068p/hsu1/Training_results/' + sim + '/' + galaxy + '/' + dim\n",
    "roc_title = sim + '_' + galaxy + '_' + dim\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 1024\n",
    "roc_path = '/ocean/projects/phy210068p/hsu1/Training_results/' + sim + '/' + galaxy + '/' + 'roc_parameters.hdf5'\n",
    "\n",
    "if transfer == True:\n",
    "    out_dir = out_dir + '_transfer'\n",
    "    transfer_checkpoint = '/ocean/projects/phy210068p/hsu1/Training_results/' + sim +'/'+ transfer_galaxy +'/'+ dim + '/training_logs/version_0/checkpoints/last.ckpt'\n",
    "    roc_title = sim + '_' + transfer_galaxy + '_on_' + galaxy\n",
    "    roc_path = '/ocean/projects/phy210068p/hsu1/Training_results/' + sim + '/' + roc_title + '_roc_parameters.hdf5'\n",
    "\n",
    "train_parameter_file = out_dir + '/training_parameters.hdf5'\n",
    "train_log = out_dir + '/training_logs'\n",
    "checkpoint = train_log + '/version_0/checkpoints/last.ckpt'\n",
    "    \n",
    "#Saving roc curves\n",
    "def save_roc(roc_path, epsilon_i, epsilon_a):\n",
    "    with h5py.File(roc_path, 'a') as f:\n",
    "        if dim + '_ep_i' in f.keys():\n",
    "            del f[dim + '_ep_i']\n",
    "            del f[dim + '_ep_a']\n",
    "            f.create_dataset(dim + '_ep_i', data=epsilon_i)\n",
    "            f.create_dataset(dim + '_ep_a', data=epsilon_a)\n",
    "        else:\n",
    "            f.create_dataset(dim + '_ep_i', data=epsilon_i)\n",
    "            f.create_dataset(dim + '_ep_a', data=epsilon_a)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f759e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "f = h5py.File(path, 'r')\n",
    "\n",
    "for i in x_keys:\n",
    "    data.append(f[i][:])\n",
    "# y = f[y_key][:]\n",
    "\n",
    "# Getting rid of nan values\n",
    "x = []\n",
    "if 'Jr' in x_keys:\n",
    "    Jr = f['Jr'][:]\n",
    "    mask = (~np.isnan(Jr))\n",
    "    for i in range(len(x_keys)):\n",
    "        new = data[i][:][mask]\n",
    "        x.append(new)\n",
    "#     y = y[mask]\n",
    "elif 'radial_velocity' in x_keys:\n",
    "    rv = f['radial_velocity'][:]\n",
    "    mask = (~np.isnan(rv))\n",
    "    for i in range(len(x_keys)):\n",
    "        new = data[i][:][mask]\n",
    "        x.append(new)\n",
    "#     y = y[mask]\n",
    "else:\n",
    "    x = data\n",
    "\n",
    "x = np.vstack(x).T\n",
    "f.close()\n",
    "\n",
    "# #Select random stars\n",
    "# np.random.seed(42)\n",
    "# select = np.random.choice(len(x), 200000)\n",
    "# x = x[select]\n",
    "# y = y[select]\n",
    "\n",
    "shuffle = np.random.permutation(len(x))\n",
    "x = x[shuffle]\n",
    "# y = y[shuffle]\n",
    "\n",
    "n_train = int(0.9 *len(x))\n",
    "n_val = len(x)-n_train\n",
    "train_x, val_x = x[:n_train], x[n_train: n_train+n_val]\n",
    "# train_y, val_y = y[:n_train], y[n_train: n_train+n_val]\n",
    "\n",
    "# ny1 = np.sum(train_y==1)\n",
    "# ny0 = np.sum(train_y==0)\n",
    "# ny = ny1 + ny0\n",
    "# w1 = ny/ny1\n",
    "# w0 = ny/ny0\n",
    "# weight = torch.tensor([w0, w1], dtype=torch.float32)\n",
    "mean_train_x = np.mean(train_x, axis = 0)\n",
    "stdv_train_x = np.std(train_x, axis = 0)\n",
    "train_x = (train_x - mean_train_x) / stdv_train_x\n",
    "val_x = (val_x - mean_train_x) / stdv_train_x\n",
    "train_x = torch.tensor(train_x, dtype=torch.float32)\n",
    "# train_y = torch.tensor(train_y, dtype=torch.long)\n",
    "val_x = torch.tensor(val_x, dtype=torch.float32)\n",
    "# val_y = torch.tensor(val_y, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(train_x)\n",
    "val_dataset = TensorDataset(val_x)\n",
    "# train_dataset = TensorDataset(train_x, train_y)\n",
    "# val_dataset = TensorDataset(val_x, val_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size = batch_size)\n",
    "# with h5py.File(train_parameter_file, 'w') as f:\n",
    "#     f.create_dataset('shuffle', data=shuffle)\n",
    "#     f.attrs['n_train']=n_train\n",
    "#     f.attrs['n_val']=n_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89d89fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(LightningModule):\n",
    "                \n",
    "    def __init__(self, weight, mean_train_x, stdv_train_x, transfer):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.l1 = torch.nn.Linear(len(x_keys), 100) \n",
    "        self.l2 = torch.nn.Linear(100, 50)\n",
    "        self.l3 = torch.nn.Linear(50, 2)\n",
    "        \n",
    "        self.train_acc = torchmetrics.Accuracy()\n",
    "        self.valid_acc = torchmetrics.Accuracy()\n",
    "        self.weight = weight\n",
    "        self.mean_train_x = mean_train_x\n",
    "        self.stdv_train_x = stdv_train_x\n",
    "        if transfer == True:\n",
    "            self.feature_extractor = Model.load_from_checkpoint(transfer_checkpoint, transfer=False)\n",
    "            self.feature_extractor.freeze()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_out = self.l1(x)\n",
    "        x_out = torch.relu(x_out)\n",
    "        x_out = self.l2(x_out)\n",
    "        x_out = torch.relu(x_out)\n",
    "        x_out = self.l3(x_out)\n",
    "        return x_out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        train_x, train_y = batch \n",
    "        preds = self(train_x)\n",
    "        loss = F.cross_entropy(preds, train_y, weight = self.weight.to(self.device))\n",
    "        self.train_acc(preds, train_y)\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', self.train_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        val_x, val_y = batch\n",
    "        preds = self(val_x)\n",
    "        loss = F.cross_entropy(preds, val_y, weight=self.weight.to(self.device))\n",
    "        self.valid_acc(preds, val_y)\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('valid_acc', self.valid_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d8dde4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /ocean/projects/phy210068p/hsu1/Training_results/DR3_lsr012/Gaia/4D_transfer/training_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type     | Params\n",
      "---------------------------------------\n",
      "0 | l1        | Linear   | 500   \n",
      "1 | l2        | Linear   | 5.0 K \n",
      "2 | l3        | Linear   | 102   \n",
      "3 | train_acc | Accuracy | 0     \n",
      "4 | valid_acc | Accuracy | 0     \n",
      "---------------------------------------\n",
      "5.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "5.7 K     Total params\n",
      "0.023     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e045604e5174d459ccc8ee01ddc482e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jet/home/hsu1/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     16\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, default_root_dir\u001b[38;5;241m=\u001b[39mout_dir,\n\u001b[1;32m     18\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks, logger\u001b[38;5;241m=\u001b[39mtrainer_logger, enable_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m Model\u001b[38;5;241m.\u001b[39mload_from_checkpoint(checkpoint)\n\u001b[1;32m     24\u001b[0m mean \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmean_train_x\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:770\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;124;03mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;124;03m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 770\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:723\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 723\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:811\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    807\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    809\u001b[0m     ckpt_path, model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    810\u001b[0m )\n\u001b[0;32m--> 811\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1236\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[0;32m-> 1236\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1238\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1323\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1345\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_training_routine()\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;66;03m# enable train mode\u001b[39;00m\n\u001b[1;32m   1348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1413\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1411\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m-> 1413\u001b[0m     \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:155\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_dataloaders \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    154\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataloader_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataloader_idx\n\u001b[0;32m--> 155\u001b[0m dl_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_max_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs\u001b[38;5;241m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:128\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# lightning module methods\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:226\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39m_call_strategy_hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 226\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1765\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1764\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1765\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1768\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:344\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m\"\"\"The actual validation step.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03mSee :meth:`~pytorch_lightning.core.lightning.LightningModule.validation_step` for more details\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mval_step_context():\n\u001b[0;32m--> 344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mModel.validation_step\u001b[0;34m(self, batch, batch_nb)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_nb):\n\u001b[0;32m---> 40\u001b[0m     val_x, val_y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     41\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(val_x)\n\u001b[1;32m     42\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(preds, val_y, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice))\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "if transfer == True:\n",
    "    model = Model.load_from_checkpoint(transfer_checkpoint, transfer=False)\n",
    "#     model = Model(weight, mean_train_x, stdv_train_x, transfer=True)\n",
    "else:\n",
    "    model = Model(weight, mean_train_x, stdv_train_x, transfer=False)\n",
    "\n",
    "# Create trainer\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        monitor=\"val_loss\", mode='min', filename=\"{epoch}-{val_loss:.4f}\",\n",
    "        save_top_k=3, save_last=True, save_weights_only=True),\n",
    "    EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=5, mode='min', verbose=True)\n",
    "]\n",
    "trainer_logger = CSVLogger(out_dir, name=train_log)\n",
    "trainer = Trainer(\n",
    "    accelerator=\"auto\", devices=1 if torch.cuda.is_available() else None,\n",
    "    max_epochs=10, default_root_dir=out_dir,\n",
    "    callbacks=callbacks, logger=trainer_logger, enable_progress_bar=True )\n",
    "\n",
    "# Start training\n",
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "model = Model.load_from_checkpoint(checkpoint)\n",
    "mean = model.mean_train_x\n",
    "stdv = model.stdv_train_x\n",
    "weight = model.weight\n",
    "\n",
    "n_test = n_val\n",
    "test_x = x[n_train: n_train+n_test]\n",
    "test_y = y[n_train: n_train+n_test]\n",
    "\n",
    "test_x = (test_x - mean) / stdv\n",
    "\n",
    "test_x = torch.tensor(test_x, dtype=torch.float32)\n",
    "test_y = torch.tensor(test_y, dtype=torch.long)\n",
    "\n",
    "test_dataset = list(zip(test_x, test_y))\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size)\n",
    "\n",
    "predict = []\n",
    "target = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x, y = batch\n",
    "        yhat = model(x)\n",
    "        predict.append(yhat.cpu().numpy())\n",
    "        target.append(y.cpu().numpy())\n",
    "predict = np.concatenate(predict)\n",
    "target = np.concatenate(target)\n",
    "\n",
    "score = np.exp(predict[:,1])/(np.exp(predict[:,0])+np.exp(predict[:,1]))\n",
    "target_true_mask = (target==True)\n",
    "target_false_mask = (target==False)\n",
    "\n",
    "thresholds = np.linspace(0.001, 1, 100)\n",
    "precision = []\n",
    "recall = []\n",
    "epsilon_a = []\n",
    "epsilon_i = []\n",
    "for thres in thresholds:\n",
    "    score_1 = score>thres\n",
    "    score_1_true_mask = (score_1==True)\n",
    "    score_1_false_mask = (score_1==False)\n",
    "    TP = np.sum(score_1[target_true_mask])\n",
    "    FP = np.sum(score_1[target_false_mask])\n",
    "    TN = np.sum(~score_1[target_false_mask])\n",
    "    FN = np.sum(~score_1[target_true_mask])\n",
    "    N_a = TP + FN\n",
    "    N_i = TN + FP\n",
    "    N_a_s = TP\n",
    "    N_i_s = FP\n",
    "    epsilon_a_thres = N_a_s / N_a\n",
    "    epsilon_i_thres = N_i_s / N_i\n",
    "    epsilon_a.append(epsilon_a_thres)\n",
    "    epsilon_i.append(epsilon_i_thres)\n",
    "\n",
    "save_roc(roc_path, epsilon_i, epsilon_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a24ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
