{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e839551e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98780/3742702038.py:10: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from IPython.core.display import display\n",
    "from collections.abc import Mapping\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "import torchmetrics\n",
    "from torchvision.transforms import ToTensor\n",
    "import astropy.units as u\n",
    "import astropy.coordinates as coord\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-colorblind')\n",
    "\n",
    "#User Input\n",
    "sim = 'DR3_lsr012' #input(\"DR2 or DR3: \")\n",
    "dim = '4D' #input(\"Input how many dimensions are needed: \")\n",
    "galaxy = 'Gaia' #input(\"Use m12i or m12f data: \")\n",
    "transfer = True #bool(input(\"Transfer learning (True or False): \"))\n",
    "if transfer == True:\n",
    "    transfer_galaxy = 'm12i' #i nput(\"Which galaxy parameters for transfer learning: \")\n",
    "\n",
    "# Training data\n",
    "if dim == '4D':\n",
    "    x_keys = ['ra', 'dec', 'pmra', 'pmdec']\n",
    "elif dim == '5D':\n",
    "    x_keys = ['ra', 'dec', 'pmra', 'pmdec', 'parallax']\n",
    "elif dim == '6D':\n",
    "    x_keys = ['ra', 'dec', 'pmra', 'pmdec', 'parallax', 'radial_velocity']\n",
    "elif dim == '7D':\n",
    "    x_keys = ['ra', 'dec', 'pmra', 'pmdec', 'parallax', 'radial_velocity', 'feh']\n",
    "elif dim == '9D':\n",
    "    x_keys = ['ra', 'dec', 'pmra', 'pmdec', 'parallax', 'radial_velocity', 'Jr', 'Jphi', 'Jz']\n",
    "elif dim == '10D':\n",
    "    x_keys = ['ra', 'dec', 'pmra', 'pmdec', 'parallax', 'radial_velocity', 'Jr', 'Jphi', 'Jz', 'feh']\n",
    "elif dim == '6D_cyl':\n",
    "    x_keys = ['ra', 'dec', 'pmra', 'pmdec', 'parallax', 'radial_velocity']\n",
    "elif dim == '6D_gal':\n",
    "    x_keys = ['ra', 'dec', 'pmra', 'pmdec', 'parallax', 'radial_velocity']\n",
    "    \n",
    "y_key = 'is_accreted'\n",
    "\n",
    "# Directories\n",
    "# path = '/ocean/projects/phy210068p/hsu1/Ananke_datasets_training/AnankeDR3_data_reduced_m12i_lsr012.hdf5'\n",
    "path = '/ocean/projects/phy210068p/hsu1/Ananke_datasets_training/GaiaDR3_data_reduced_feh.hdf5'\n",
    "out_dir = '/ocean/projects/phy210068p/hsu1/Training_results/' + sim + '/' + galaxy + '/' + dim\n",
    "roc_title = sim + '_' + galaxy + '_' + dim\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 1024\n",
    "roc_path = '/ocean/projects/phy210068p/hsu1/Training_results/' + sim + '/' + galaxy + '/roc_parameters.hdf5'\n",
    "training_score_path = '/ocean/projects/phy210068p/hsu1/Training_results/' + sim + '/' + galaxy + '/' + dim + '/training_score.hdf5'\n",
    "\n",
    "if transfer == True:\n",
    "    transfer_checkpoint = '/ocean/projects/phy210068p/hsu1/Training_results/' + sim +'/'+ transfer_galaxy +'/'+ dim + '/training_logs/version_0/checkpoints/last.ckpt'\n",
    "\n",
    "train_parameter_file = out_dir + '/training_parameters.hdf5'\n",
    "train_log = out_dir + '/training_logs'\n",
    "checkpoint = train_log + '/version_0/checkpoints/last.ckpt'\n",
    "    \n",
    "#Saving roc curves\n",
    "def save_roc(roc_path, epsilon_i, epsilon_a):\n",
    "    with h5py.File(roc_path, 'a') as f:\n",
    "        if dim + '_ep_i' in f.keys():\n",
    "            del f[dim + '_ep_i']\n",
    "            del f[dim + '_ep_a']\n",
    "            f.create_dataset(dim + '_ep_i', data=epsilon_i)\n",
    "            f.create_dataset(dim + '_ep_a', data=epsilon_a)\n",
    "        else:\n",
    "            f.create_dataset(dim + '_ep_i', data=epsilon_i)\n",
    "            f.create_dataset(dim + '_ep_a', data=epsilon_a)\n",
    "    # with h5py.File(training_score_path, 'w') as f:\n",
    "    #         f.create_dataset('score', data=score)\n",
    "    #         f.create_dataset('target', data=target)\n",
    "    #         f.create_dataset('x_final', data=x_final)\n",
    "            \n",
    "# Loading data\n",
    "data = []\n",
    "f = h5py.File(path, 'r')\n",
    "\n",
    "for i in x_keys:\n",
    "    data.append(f[i][:])\n",
    "y = f[y_key][:]\n",
    "\n",
    "# Getting rid of nan values\n",
    "x = []\n",
    "if 'Jr' in x_keys:\n",
    "    Jr = f['Jr'][:]\n",
    "    mask = (~np.isnan(Jr))\n",
    "    for i in range(len(x_keys)):\n",
    "        new = data[i][:][mask]\n",
    "        x.append(new)\n",
    "    y = y[mask]\n",
    "elif 'radial_velocity' in x_keys:\n",
    "    rv = f['radial_velocity'][:]\n",
    "    mask = (~np.isnan(rv))\n",
    "    for i in range(len(x_keys)):\n",
    "        new = data[i][:][mask]\n",
    "        x.append(new)\n",
    "    y = y[mask]\n",
    "else:\n",
    "    x = data\n",
    "\n",
    "x = np.vstack(x).T\n",
    "f.close()\n",
    "\n",
    "# #Select random stars\n",
    "# np.random.seed(42)\n",
    "# select = np.random.choice(len(x), 200000)\n",
    "# x = x[select]\n",
    "# y = y[select]\n",
    "\n",
    "# Splitting data into training and validation sets\n",
    "\n",
    "shuffle = np.random.permutation(len(x))\n",
    "x = x[shuffle]\n",
    "y = y[shuffle]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18d90b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.93954854e+02, -6.17387437e+01, -7.10701343e+00,\n",
       "         -1.31862166e+00],\n",
       "        [ 1.44627646e+02,  4.04484325e+01, -5.38654077e-01,\n",
       "         -7.30829765e+00],\n",
       "        [ 2.34278022e+02, -5.06857174e+01,  6.17753623e+00,\n",
       "          7.47318672e+00],\n",
       "        ...,\n",
       "        [ 1.38816922e+02, -5.93519969e+01, -9.75051529e-01,\n",
       "          2.13861768e+00],\n",
       "        [ 2.95209459e+02,  1.69356430e+01, -7.20719984e+00,\n",
       "         -1.39369629e+01],\n",
       "        [ 1.36872930e+01,  6.32641805e+01, -2.86000853e+00,\n",
       "         -2.57086442e-01]]),\n",
       " array([False, False, False, ..., False, False, False]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_saved = x\n",
    "y_saved = y\n",
    "x.shape, y.shape\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d4118c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[0:1000]\n",
    "y = y[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18ca5612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 4), (1000,), (90172126, 4), (90172126,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape, x_saved.shape, y_saved.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d92c1ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98780/1948919957.py:11: RuntimeWarning: divide by zero encountered in long_scalars\n",
      "  w1 = ny/ny1\n"
     ]
    }
   ],
   "source": [
    "# %90 training, %10 validation\n",
    "n_train = int(0.9 *len(x))\n",
    "n_val = len(x)-n_train\n",
    "train_x, val_x = x[:n_train], x[n_train: n_train+n_val]\n",
    "train_y, val_y = y[:n_train], y[n_train: n_train+n_val]\n",
    "\n",
    "ny1 = np.sum(train_y==1)\n",
    "ny0 = np.sum(train_y==0)\n",
    "ny = ny1 + ny0\n",
    "# Weights for cross entropy loss\n",
    "w1 = ny/ny1\n",
    "w0 = ny/ny0\n",
    "weight = torch.tensor([w0, w1], dtype=torch.float32)\n",
    "mean_train_x = np.mean(train_x, axis = 0)\n",
    "stdv_train_x = np.std(train_x, axis = 0)\n",
    "# Normalizing data\n",
    "train_x = (train_x - mean_train_x) / stdv_train_x\n",
    "val_x = (val_x - mean_train_x) / stdv_train_x\n",
    "train_x = torch.tensor(train_x, dtype=torch.float32)\n",
    "train_y = torch.tensor(train_y, dtype=torch.long)\n",
    "val_x = torch.tensor(val_x, dtype=torch.float32)\n",
    "val_y = torch.tensor(val_y, dtype=torch.long)\n",
    "\n",
    "# Creating dataloaders\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "val_dataset = TensorDataset(val_x, val_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size = batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size = batch_size)\n",
    "\n",
    "# Saving training parameters\n",
    "with h5py.File(train_parameter_file, 'w') as f:\n",
    "    f.create_dataset('shuffle', data=shuffle)\n",
    "    f.attrs['n_train']=n_train\n",
    "    f.attrs['n_val']=n_val\n",
    "\n",
    "# Creating model\n",
    "class Model(LightningModule):\n",
    "                \n",
    "    def __init__(self, weight, mean_train_x, stdv_train_x, transfer):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.l1 = torch.nn.Linear(len(x_keys), 100) \n",
    "        self.l2 = torch.nn.Linear(100, 50)\n",
    "        self.l3 = torch.nn.Linear(50, 2)\n",
    "        \n",
    "        self.train_acc = torchmetrics.Accuracy()\n",
    "        self.valid_acc = torchmetrics.Accuracy()\n",
    "        self.weight = weight\n",
    "        self.mean_train_x = mean_train_x\n",
    "        self.stdv_train_x = stdv_train_x\n",
    "        # In case of transfer learning, freeze the feature extractor\n",
    "        if transfer == True:\n",
    "            self.feature_extractor = Model.load_from_checkpoint(transfer_checkpoint, transfer=False)\n",
    "            self.feature_extractor.freeze()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_out = self.l1(x)\n",
    "        x_out = torch.relu(x_out)\n",
    "        x_out = self.l2(x_out)\n",
    "        x_out = torch.relu(x_out)\n",
    "        x_out = self.l3(x_out)\n",
    "        return x_out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        train_x, train_y = batch \n",
    "        preds = self(train_x)\n",
    "        loss = F.cross_entropy(preds, train_y, weight = self.weight.to(self.device))\n",
    "        self.train_acc(preds, train_y)\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', self.train_acc, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        val_x, val_y = batch\n",
    "        preds = self(val_x)\n",
    "        loss = F.cross_entropy(preds, val_y, weight=self.weight.to(self.device))\n",
    "        self.valid_acc(preds, val_y)\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('valid_acc', self.valid_acc, on_step=False, on_epoch=True, prog_bar=False)\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eedcd83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /ocean/projects/phy210068p/hsu1/Training_results/DR3_lsr012/Gaia/4D/training_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type     | Params\n",
      "-----------------------------------------------\n",
      "0 | l1                | Linear   | 500   \n",
      "1 | l2                | Linear   | 5.0 K \n",
      "2 | l3                | Linear   | 102   \n",
      "3 | train_acc         | Accuracy | 0     \n",
      "4 | valid_acc         | Accuracy | 0     \n",
      "5 | feature_extractor | Model    | 5.7 K \n",
      "-----------------------------------------------\n",
      "5.7 K     Trainable params\n",
      "5.7 K     Non-trainable params\n",
      "11.3 K    Total params\n",
      "0.045     Total estimated model params size (MB)\n",
      "/jet/home/hsu1/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/jet/home/hsu1/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/jet/home/hsu1/anaconda3/envs/torch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1933: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "Metric val_loss improved. New best score: 0.741\n",
      "Metric val_loss improved by 0.037 >= min_delta = 0.0. New best score: 0.705\n",
      "Metric val_loss improved by 0.035 >= min_delta = 0.0. New best score: 0.670\n",
      "Metric val_loss improved by 0.034 >= min_delta = 0.0. New best score: 0.636\n",
      "Metric val_loss improved by 0.033 >= min_delta = 0.0. New best score: 0.603\n",
      "Metric val_loss improved by 0.032 >= min_delta = 0.0. New best score: 0.571\n",
      "Metric val_loss improved by 0.031 >= min_delta = 0.0. New best score: 0.539\n",
      "Metric val_loss improved by 0.031 >= min_delta = 0.0. New best score: 0.509\n",
      "Metric val_loss improved by 0.030 >= min_delta = 0.0. New best score: 0.478\n",
      "Metric val_loss improved by 0.030 >= min_delta = 0.0. New best score: 0.449\n",
      "/tmp/ipykernel_98780/2548767490.py:81: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  epsilon_a_thres = N_a_s / N_a\n"
     ]
    }
   ],
   "source": [
    "#  Load different models in LightningModule based on transfer learning\n",
    "if transfer == True:\n",
    "    model = Model(weight, mean_train_x, stdv_train_x, transfer=True)\n",
    "else:\n",
    "    model = Model(weight, mean_train_x, stdv_train_x, transfer=False)\n",
    "\n",
    "# Create trainer\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        monitor=\"val_loss\", mode='min', filename=\"{epoch}-{val_loss:.4f}\",\n",
    "        save_top_k=3, save_last=True, save_weights_only=True),\n",
    "    EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=5, mode='min', verbose=True)\n",
    "]\n",
    "trainer_logger = CSVLogger(out_dir, name=train_log)\n",
    "trainer = Trainer(\n",
    "    accelerator=\"auto\", devices=1 if torch.cuda.is_available() else None,\n",
    "    max_epochs=10, default_root_dir=out_dir,\n",
    "    callbacks=callbacks, logger=trainer_logger, enable_progress_bar=False )\n",
    "\n",
    "# Start training\n",
    "trainer.fit(model=model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "\n",
    "# Load best model\n",
    "model = Model.load_from_checkpoint(checkpoint)\n",
    "mean = model.mean_train_x\n",
    "stdv = model.stdv_train_x\n",
    "weight = model.weight\n",
    "\n",
    "# Load test data\n",
    "\n",
    "test_x = (x - mean) / stdv\n",
    "\n",
    "test_x = torch.tensor(test_x, dtype=torch.float32)\n",
    "test_y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "test_dataset = list(zip(test_x, test_y))\n",
    "test_loader = DataLoader(test_dataset, batch_size = batch_size)\n",
    "\n",
    "# Get predictions\n",
    "predict = []\n",
    "target = []\n",
    "x_final = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x, y = batch\n",
    "        yhat = model(x)\n",
    "        predict.append(yhat.cpu().numpy())\n",
    "        target.append(y.cpu().numpy())\n",
    "        x_final.append(x.cpu().numpy())\n",
    "predict = np.concatenate(predict)\n",
    "target = np.concatenate(target)\n",
    "x_final = np.concatenate(x_final)\n",
    "\n",
    "# Calculate scores based on predictions\n",
    "\n",
    "score = np.exp(predict[:,1])/(np.exp(predict[:,0])+np.exp(predict[:,1]))\n",
    "target_true_mask = (target==True)\n",
    "target_false_mask = (target==False)\n",
    "\n",
    "# Calculate ROC curve for 1000 thresholds\n",
    "\n",
    "thresholds = np.linspace(0.001, 1, 1000)\n",
    "precision = []\n",
    "recall = []\n",
    "epsilon_a = []\n",
    "epsilon_i = []\n",
    "for thres in thresholds:\n",
    "    score_1 = score>thres\n",
    "    score_1_true_mask = (score_1==True)\n",
    "    score_1_false_mask = (score_1==False)\n",
    "    TP = np.sum(score_1[target_true_mask])\n",
    "    FP = np.sum(score_1[target_false_mask])\n",
    "    TN = np.sum(~score_1[target_false_mask])\n",
    "    FN = np.sum(~score_1[target_true_mask])\n",
    "    N_a = TP + FN\n",
    "    N_i = TN + FP\n",
    "    N_a_s = TP\n",
    "    N_i_s = FP\n",
    "    epsilon_a_thres = N_a_s / N_a\n",
    "    epsilon_i_thres = N_i_s / N_i\n",
    "    epsilon_a.append(epsilon_a_thres)\n",
    "    epsilon_i.append(epsilon_i_thres)\n",
    "\n",
    "# Plot and save ROC curve data\n",
    "save_roc(roc_path, epsilon_i, epsilon_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88090018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
